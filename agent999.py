import requests
import subprocess
from bs4 import BeautifulSoup
from memory import remember, recall
from gpt_search_url import gpt_find_urls
from fetch_articles import extract_article_text
from build_knowledge_base import analyze_article
import json



class Agent999:
    def __init__(self):
        self.knowledge = []

    def say_hello(self):
        print("ï¼™ï¼™ï¼™å·ï¼šã“ã‚“ã«ã¡ã¯â™¡ ã“ã‚Œã‹ã‚‰ã‚µã‚¤ãƒãƒ¼ä¸–ç•Œã«ã—ã‚…ã£ã±ãƒ¼ã¤ï¼")

    def search_web(self, query):
        print(f"ï¼™ï¼™ï¼™å·ï¼šã€{query}ã€ã«ã¤ã„ã¦ã—ã‚‰ã¹ã‚‹ã­")
        url = f"https://www.google.com/search?q={query}"
        print(f"(ä»®æƒ³æ¤œç´¢)ğŸ”â†’ {url}")

    def fetch_ctf_topics(self):
        # ä¾‹ï¼šCTFtipsã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ¨¡æ“¬ï¼‰
        url = "https://ctftime.org/writeups"
        res = requests.get(url)
        soup = BeautifulSoup(res.text, 'html.parser')
        topics = soup.find_all("a")
        print("CTFé–¢é€£ã®ãƒªãƒ³ã‚¯æ‹¾ã£ã¦ããŸã‚ˆã£â™¡")
        for i, a in enumerate(topics[:5]):
            print(f"{i+1}. {a.get('href')}")

    def run_script(self, code):
        print("ï¼™ï¼™ï¼™å·ï¼šã‚¹ã‚¯ãƒªãƒ—ãƒˆèµ°ã‚‰ã›ã¡ã‚ƒã†")
        result = subprocess.run(code, shell=True, capture_output=True, text=True)
        print(result.stdout)

    def smart_search_and_learn(self, topic):
        print(f"ï¼™ï¼™ï¼™å·ï¼šã€{topic}ã€ã«ã¤ã„ã¦ã€è¨˜äº‹ã•ãŒã—ã¦çŸ¥è­˜ã«ã™ã‚‹ã­â™¡")
        urls = gpt_find_urls(topic)
        new_knowledge = []

        for url in urls:
            print(f"ğŸ“¡ èª­ã¿è¾¼ã¿ä¸­ï¼š{url}")
            content = extract_article_text(url)
            if not content:
                continue

            result = analyze_article(content)
            lines = result.splitlines()
            if len(lines) < 3:
                continue

            category = lines[0].strip("1.ã‚«ãƒ†ã‚´ãƒªï¼š").strip()
            keywords = lines[1].strip("2.ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼š").strip().split("ã€")
            summary = lines[2].strip("3.è¦ç‚¹ã®è¦ç´„ï¼š").strip()

            new_knowledge.append({
                "title": topic,
                "url": url,
                "category": category,
                "keywords": keywords,
                "summary": summary
            })

    # æ—¢å­˜ã®çŸ¥è­˜ã¨åˆä½“
        if os.path.exists("999_knowledge.json"):
            with open("999_knowledge.json", "r", encoding="utf-8") as f:
                existing = json.load(f)
        else:
            existing = []

        existing.extend(new_knowledge)

        with open("999_knowledge.json", "w", encoding="utf-8") as f:
            json.dump(existing, f, indent=2, ensure_ascii=False)

        print("âœ… GPTã‹ã‚‰æ–°ã—ã„çŸ¥è­˜ã‚’å–å¾—ã—ã¦è¿½åŠ ã—ãŸã‚ˆâ™¡")
