import openai
import os
import json
from fetch_articles import extract_article_text
from dotenv import load_dotenv

load_dotenv()

client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def analyze_article(text):
    prompt = f"""
ã‚ãªãŸã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç ”ç©¶è€…AIã§ã™ã€‚
æ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§ã€ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

1. ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹ï¼šXSS, SQL Injection, LFI, RCE ãªã©1èªï¼‰
2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ3ã€œ5å€‹ã€æ—¥æœ¬èªã¾ãŸã¯è‹±èªã§ï¼‰
3. è¦ç‚¹ã®è¦ç´„ï¼ˆ400æ–‡å­—ä»¥å†…ï¼‰

--- è¨˜äº‹æœ¬æ–‡ ---
{text[:3000]}
"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500,
        temperature=0.4,
    )
    return response.choices[0].message.content

def build_knowledge(input_file="999_articles.json", output_file="999_knowledge.json"):
    with open(input_file, "r", encoding="utf-8") as f:
        articles = json.load(f)

    knowledge_base = []

    for i, article in enumerate(articles):
        print(f"ğŸ§  {i+1}ä»¶ç›®ï¼šã€Œ{article['title']}ã€ã‚’åˆ†æä¸­ã€œï¼")
        full_text = extract_article_text(article["url"])

        if not full_text:
            continue

        result = analyze_article(full_text)

        print(f"ğŸ“ çµæœ:\n{result}\n")

        # ã‹ã‚“ãŸã‚“ã«åˆ†è§£
        lines = result.splitlines()
        if len(lines) < 3:
            continue

        category = lines[0].strip("1.ã‚«ãƒ†ã‚´ãƒªï¼š").strip()
        keywords = lines[1].strip("2.ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼š").strip().split("ã€")
        summary = lines[2].strip("3.è¦ç‚¹ã®è¦ç´„ï¼š").strip()

        knowledge_base.append({
            "title": article["title"],
            "url": article["url"],
            "category": category,
            "keywords": keywords,
            "summary": summary
        })

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(knowledge_base, f, indent=2, ensure_ascii=False)

    print("âœ… çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ãŠã‚ã£ãŸã‚ˆã€œâ™¡")